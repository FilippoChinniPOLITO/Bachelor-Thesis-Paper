\chapter{Conclusions}

The results of the experiments allowed to reach significant conclusions about Hyperparamter Optimization and its related techniques. The work was able to outline the importance of HPO and evaluate its effectiveness in various contexts with the use of popular and new techniques.

\section{Summary of the Results}

In Experiment 1, it was possible to demonstrate that HPO is a powerful strategy to improve the performance of machine learning models; improvements in performance were clearly observable in the results of the experiments, showing that emphirical or random approaches to the Tuning of Hyperparameters are not as effective as a formal optimization process such as HPO.
\\[0.3cm]In Experiment 2, the applicability of a HPO's library, Optuna, was tested and evaluated. In particular, different HPO algorithms (those with an implementation in Optuna) were tested and compared on the classic MNIST dataset. The results showed that the TPE sampler was the most effective in this case, but it is vital to note that the performance of the algorithms may vary depending on the dataset and the model being used. It was also observed that a simple method, such as Random Seach, can be utilized as a baseline for comparison with other HPO algorithms.
\\[0.3cm]In Experiment 3, using an approach basically identical to Optuna's, a framework for HPO's was built, that used Particle Swarm Optimization (PSO) as the sampling algorithm. The results showed that the PSO algorithm was not as effective as the TPE sampler from Optuna, but it was still able to improve the performance of the models, reaching similar levels of quality performance-wise. The experiment also demonstrated that building a framework for HPO is a viable strategy, and can save time and effort in the preparation phase of the Hyperparameters Tuning process.
\\[0.3cm]In Experiment 4, a sampler using the PSO algorithm was implemented in Optuna. Firstly, the implementation of the sampler was revealed as a success, integrating the PSO algorithm into the Optuna library. Secondly, the results of the experiments showed that the created PSO sampler reached similar performance levels to the TPE sampler, even surpassing it in some cases. This further demonstrated the effectiveness of the PSO algorithm as a sampling method for HPO.
\\[0.3cm]In Experiment 5, the strategies utilized so far were tested on a more complex dataset, the Weed Map dataset. The results allowed in the first place not only to find the best hyperparameters for the WeedMap problem, but also to simplify the architecture of the Nueral Network used in the experiment, with a relatively small loss in the performance of the model. The results also showed, not without struggles, that the HPO algorithms were also able to work effectively in a more complex situation, in particular, PSO can now be considered a valid alternative to the TPE sampler in the Optuna library, and, in general, a top-level sampling technique in HPO. 

\section{Limitations}

There are some limitations in the presented results that need to be considered. These limitations are mainly related to the intrinsic complexity of HPO from a computational cost point of view.
\\[0.3cm]In general, all strategies revealed themselves effective only when a large number of trials were performed, and obviously, the more trials are performed, the more time is needed, and the more time is needed, the more computational resources are required to make the process feasible. This limitation is particularly evident when using the PSO sampler, which showed, most of the time, to require more trials than the TPE sampler to reach a satisfactory level of performance.
The limitation did not impact the results of the first four experiments, as both the model and the dataset were simple, therefore requiring less time to complete a single trial, and fewer total trials. The problems, however, became evident in the last experiment, where the complexity of the dataset and the model required a large number of trials to reach a good level of performance. Even with relatively good, but not excellent, hardware, the time required to complete the trials was significantly higher than in the previous experiments, and this led to the necessity of reducing the number of trials to perform, so as to make the process feasible in a reasonable amount of time (below weeks).
\\[0.3cm]Another limitation to be considered, is the lack of a more detailed analysis of other HPO algorithms, starting with the ones implemented in Optuna, which were given less importance in the experiments. As for the objectives of the experiments, it was not really necessary to test all the algorithms available in Optuna deeply, therefore, when one technique showed problems, for the sake of simplicity, it was decided to ignore it and move on. This limitation actually reduces the generalizability of the results, especially for what concerns Experiment 2.

\section{Future Work}

Potential future work should focus on addressing the limitations of the current work and further exploring the potential of HPO and its related techniques.
\\[0.3cm]A first recommendation is to reproduce Experiments 2 and 4 on a more complex dataset, additionally, more HPO algorithms should be tested and compared. Those might be: samplers already implemented in Optuna; existing algorithms to adapt for HPO, implementing them from scratch; creating brand new HPO algorithms.
\\[0.3cm]A second recommendation is to execute Experiment 5 on top-level hardware, in order to execute the optimization with more trials. This would allow to reach a better level of performance, and better compare the PSO sampler with the TPE sampler.
\\[0.3cm]A third recommendation concerns the PSO algorithm for HPO. The results of Experiment 5 showed how the PSOSampler suffers from Pruning more than other samplers. Future work should focus on developing specialized pruning techniques for the PSO algorithm, for instance, a pruning technique less aggressive toward particles far from the Global Optimum. Alternatively, a Pruning technique that executes a single pruning process for each particle individually. 
\\[0.3cm]The results of Experiment 4 are a fundamental baseline for a potential pull-request to the Optuna library, in order to integrate the PSO sampler into the official release of the library. This would make the PSO sampler available to a wider audience. Before doing this, however, it is recommended to further test the sampler on different datasets and models, in order to ensure its effectiveness in different contexts.
\\[0.3cm]Hyperparamter Optimization is a sub-field of Machine Learning that is still in its infancy. Its main limitation lies in the computational cost required to perform the optimization. In the future, hardware might be able to overcome this limitation, by then, research in HPO should focus on the development of more efficient algorithms, adapting already existing optimization techniques, or creating new ones specifically designed for HPO.   
