@misc{Tesi-1.1,
  author = {{Wikipedia contributors}},
  title  = {Hyperparameter optimization --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2024},
  url    = {https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=1193594191},
  note   = {[Online; accessed 14-May-2024]}
}
@misc{Tesi-1.2,
	title 	     = {{Hyperparameter Optimization in Machine Learning Models}},
	author       = {Sayak Paul},
	year		 = 2021,
	publisher	 = {Datacamp.com},
	url			 = {https://www.datacamp.com/tutorial/parameter-optimization-machine-learning-models},
}
@misc{Tesi-1.3,
	title 	     = {{Automatic Hyperparameter Tuning - A Visual Guide}},
	author       = {Antonin Raffin},
	year		 = 2023,
	url			 = {https://araffin.github.io/post/hyperparam-tuning/},
}
@misc{Tesi-1.4,
	title 	     = {{Hyperparameter Optimization With Random Search and Grid Search}},
	author       = {Jason Brownlee},
	year		 = 2020,
	publisher	 = {Machine Learning Mastery},
	journal		 = {Python Machine Learning},
	url			 = {https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/},
}
@misc{Tesi-1.5,
	title 	     = {{Python Optuna: A Guide to Hyperparameter Optimization}},
	author       = {datagy.io},
	year		 = 2023,
	publisher	 = {datagy.io},
	url			 = {https://datagy.io/python-optuna/},
}
@book{Tesi-1.6,
	title        = {{Dive into Deep Learning}},
	author       = {Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
	year         = 2023,
	publisher    = {Cambridge University Press},
	chapter      = 19,
	url			 = {https://d2l.ai/chapter_hyperparameter-optimization/index.html},
}
@misc{Tesi-1.7,
	title 	     = {{Tuning the hyper-parameters of an estimator}},
	author       = {scikit-learn developers},
	publisher	 = {scikit-learn},
	year		 = 2020,
	url			 = {https://scikit-learn.org/stable/modules/grid_search.html},
}
@article{Tesi-1.8,
	author 		 = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	title 		 = {Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
	journal 	 = {WIREs Data Mining and Knowledge Discovery},
	volume 		 = {13},
	number 		 = {2},
	pages 		 = {e1484},
	keywords 	 = {automl, hyperparameter optimization, machine learning, model selection, tuning},
	doi 		 = {https://doi.org/10.1002/widm.1484},
	url 		 = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1484},
	eprint 		 = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1484},
	abstract 	 = {Abstract Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods—for example, based on resampling error estimation for supervised machine learning—can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under: Algorithmic Development > Statistics Technologies > Machine Learning Technologies > Prediction},
	year 		 = {2023}
}
@article{Tesi-1.9,
    title		 = {Hyper-parameter optimization: A review of algorithms and applications},
  	author		 = {Yu, Tong and Zhu, Hong},
  	journal		 = {arXiv preprint arXiv:2003.05689},
  	year		 = {2020}
}
@article{Tesi-1.10,
    title 		 = {On hyperparameter optimization of machine learning algorithms: Theory and practice},
    journal 	 = {Neurocomputing},
	volume 	 	 = {415},
	pages 		 = {295-316},
	year 		 = {2020},
	issn 		 = {0925-2312},
	doi 		 = {https://doi.org/10.1016/j.neucom.2020.07.061},
	url 		 = {https://www.sciencedirect.com/science/article/pii/S0925231220311693},
	author 		 = {Li Yang and Abdallah Shami},
	keywords 	 = {Hyper-parameter optimization, Machine learning, Bayesian optimization, Particle swarm optimization, Genetic algorithm, Grid search},
	abstract 	 = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.}
}
@article{Tesi-2.1,
  author  = {I. Sa and M. Popovic and R. Khanna and Z. Chen and P. Lottes and F. Liebisch and J. Nieto and C. Stachniss and A. Walter and R. Siegwart},
  journal = {MDPI Remote Sensing},
  title   = {WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming},
  year    = {2018},
  volume  = {10},
  number  = {9},
  doi     = {doi: 10.3390/rs10091423},
  month   = {Aug}
}
@misc{Tesi-3.1,
	title 	     = {{A Gentle Introduction to Particle Swarm Optimization}},
	author       = {Adrian Tam},
	year		 = 2021,
	publisher	 = {Machine Learning Mastery},
	journal		 = {Python Machine Learning},
	url			 = {https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/},
}
@misc{Tesi-3.2,
	title 	     = {{Particle Swarm Optimization (PSO) Visually Explained}},
	author       = {Axel Thevenot},
	year		 = 2020,
	publisher	 = {Medium},
	journal		 = {Towards Data Science},
	url			 = {https://towardsdatascience.com/particle-swarm-optimization-visually-explained-46289eeb2e14},
}
@inproceedings{Tesi-3.3,
  author    = {Kennedy, J. and Eberhart, R.},
  booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
  title     = {Particle swarm optimization},
  year      = {1995},
  volume    = {4},
  number    = {},
  pages     = {1942-1948 vol.4},
  keywords  = {Particle swarm optimization;Birds;Educational institutions;Marine animals;Testing;Humans;Genetic algorithms;Optimization methods;Artificial neural networks;Performance evaluation},
  doi       = {10.1109/ICNN.1995.488968}
}
@inproceedings{Tesi-3.4,
  author    = {Eberhart and Yuhui Shi},
  booktitle = {Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546)},
  title     = {Particle swarm optimization: developments, applications and resources},
  year      = {2001},
  volume    = {1},
  number    = {},
  pages     = {81-86 vol. 1},
  keywords  = {Particle swarm optimization;Application software;Acceleration;Particle tracking;Computer science;Books;Bibliographies;Power system dynamics;Evolutionary computation;Writing},
  doi       = {10.1109/CEC.2001.934374}
}
@article{Tesi-3.5,
  author  = {Wang, Dongshu and Tan, Dapei and Liu, Lei},
  year    = {2018},
  month   = {01},
  pages   = {},
  title   = {Particle swarm optimization algorithm: an overview},
  volume  = {22},
  journal = {Soft Computing},
  doi     = {10.1007/s00500-016-2474-6}
}
@article{OptunaSamplers-TreeStructuredParzenEstimator,
  title   = {Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance},
  author  = {Watanabe, Shuhei},
  journal = {arXiv preprint arXiv:2304.11127},
  year    = {2023}
}
@article{OptunaSamplers-RandomSearch,
  author     = {Bergstra, James and Bengio, Yoshua},
  title      = {Random search for hyper-parameter optimization},
  year       = {2012},
  issue_date = {3/1/2012},
  publisher  = {JMLR.org},
  volume     = {13},
  number     = {null},
  issn       = {1532-4435},
  abstract   = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  journal    = {J. Mach. Learn. Res.},
  month      = {feb},
  pages      = {281-305},
  numpages   = {25},
  keywords   = {response surface modeling, neural networks, model selection, global optimization, deep learning}
}
@article{OptunaSamplers-NSGAII,
  author   = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  journal  = {IEEE Transactions on Evolutionary Computation},
  title    = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
  year     = {2002},
  volume   = {6},
  number   = {2},
  pages    = {182-197},
  keywords = {Genetic algorithms;Sorting;Computational complexity;Evolutionary computation;Computational modeling;Testing;Decision making;Associate members;Diversity reception;Constraint optimization},
  doi      = {10.1109/4235.996017}
}
@article{OptunaSamplers-CMA-ES,
  author    = {Hansen, N. and Ostermeier, A.},
  booktitle = {Proceedings of IEEE International Conference on Evolutionary Computation},
  title     = {Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation},
  year      = {1996},
  volume    = {},
  number    = {},
  pages     = {312-317},
  keywords  = {Genetic mutations;Covariance matrix;Evolutionary computation;Electronic switching systems;Stochastic processes},
  doi       = {10.1109/ICEC.1996.542381}
}
@inproceedings{OptunaPruners-SuccessiveHalvingAlgorithm,
  title     = {Almost Optimal Exploration in Multi-Armed Bandits},
  author    = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {1238--1246},
  year      = {2013},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  volume    = {28},
  number    = {3},
  series    = {Proceedings of Machine Learning Research},
  address   = {Atlanta, Georgia, USA},
  month     = {17--19 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v28/karnin13.pdf},
  url       = {https://proceedings.mlr.press/v28/karnin13.html},
  abstract  = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.  }
}
@inproceedings{OptunaPruners-AsynchronousSuccessiveHalvingAlgorithm,
  author    = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  booktitle = {Proceedings of Machine Learning and Systems},
  editor    = {I. Dhillon and D. Papailiopoulos and V. Sze},
  pages     = {230--246},
  title     = {A System for Massively Parallel Hyperparameter Tuning},
  url       = {https://proceedings.mlsys.org/paper_files/paper/2020/file/a06f20b349c6cf09a6b171c71b88bbfc-Paper.pdf},
  volume    = {2},
  year      = {2020}
}
@article{OptunaPruners-Hyperband,
  author  = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1--52},
  url     = {http://jmlr.org/papers/v18/16-558.html}
}

@article{AutomatedML,
  title   = {Automated machine learning: State-of-the-art and open challenges},
  author  = {Elshawi, Radwa and Maher, Mohamed and Sakr, Sherif},
  journal = {arXiv preprint arXiv:1906.02287},
  year    = {2019}
}
@article{SwarmIntelligence,
  title   = {Swarms, phase transitions, and collective intelligence},
  author  = {Millonas, Mark M},
  journal = {arXiv preprint adap-org/9306002},
  year    = {1993}
}
@article{GradientOptimization,
  title   = {Stochastic hyperparameter optimization through hypernetworks},
  author  = {Lorraine, Jonathan and Duvenaud, David},
  journal = {arXiv preprint arXiv:1802.09419},
  year    = {2018}
}

@misc{WeedMap-Repository,
  author    = {De Marinis, Pasquale},
  title     = {LWViTs-for-weedmapping},
  year      = {2023},
  publisher = {GitHub},
  journal   = {GitHub repository},
  url       = {https://github.com/pasqualedem/LWViTs-for-weedmapping}
}
@article{WeedMap-PaperThesis,
  title    = {Weed mapping in multispectral drone imagery using lightweight vision transformers},
  journal  = {Neurocomputing},
  volume   = {562},
  pages    = {126914},
  year     = {2023},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2023.126914},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223010378},
  author   = {Giovanna Castellano and Pasquale {De Marinis} and Gennaro Vessio},
  keywords = {Computer vision, Deep learning, Drones, Precision agriculture, Semantic segmentation, Weed mapping, UAV},
  abstract = {In precision agriculture, non-invasive remote sensing can be used to observe crops and weeds in visible and non-visible spectra. This paper proposes a novel approach for weed mapping using lightweight Vision Transformers. The method uses a lightweight Transformer architecture to process high-resolution aerial images obtained from drones and performs semantic segmentation to distinguish between crops and weeds. The method also employs specific architectural designs to enable transfer learning from RGB weights in a multispectral setting. For this purpose, the WeedMap dataset, acquired by drones equipped with multispectral cameras, was used. The experimental results demonstrate the effectiveness of the proposed method, exceeding the state-of-the-art. Our approach also enables more efficient mapping, allowing farmers to quickly and easily identify infested areas and prioritize their control efforts. These results encourage using drones as versatile computer vision flying devices for herbicide management, thereby improving crop yields. The code is available at https://github.com/pasqualedem/LWViTs-for-weedmapping.}
}
@inproceedings{FocalLoss,
  title     = {Focal loss for dense object detection},
  author    = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2980--2988},
  year      = {2017}
}
@misc{Repository-THESIS,
  author    = {Chinni Carella, Filippo},
  title     = {Bachelor-Thesis},
  year      = {2024},
  publisher = {GitHub},
  journal   = {GitHub repository},
  url       = {https://github.com/FilippoChinniUNIBA/Bachelor-Thesis}
}

@article{MNIST,
  title     = {The mnist database of handwritten digit images for machine learning research},
  author    = {Deng, Li},
  journal   = {IEEE Signal Processing Magazine},
  volume    = {29},
  number    = {6},
  pages     = {141--142},
  year      = {2012},
  publisher = {IEEE}
}


%Software Citations

@inproceedings{Optuna,
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  year      = {2019}
}
@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}
@incollection{PyTorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

